{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diffusion Models on Manifold Data\n",
        "\n",
        "Verification experiments comparing x-prediction, ε-prediction, and v-prediction for diffusion models trained on low-dimensional manifold data embedded in high-dimensional space.\n",
        "\n",
        "**Setup**: 2D manifold data (two moons or swiss roll) projected to D ∈ {2, 8, 16, 512, 1024} dimensions\n",
        "\n",
        "**Key test**: D=512 where model is under-complete (D > hidden_dim=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.datasets import make_moons, make_swiss_roll\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_manifold_data(n_samples: int, true_d: int, D: int, noise: float = 0.00, d_type: str = 'other', device: str = 'cpu'):\n",
        "    \"\"\"Generate toy manifold data.\n",
        "    \n",
        "    Args:\n",
        "        n_samples: Number of samples\n",
        "        true_d: True dimension (manifold dimension)\n",
        "        D: Observed dimension (ambient dimension)\n",
        "        noise: Noise level for two moons\n",
        "        d_type: Type of manifold ('two_moons' or 'swiss_roll')\n",
        "        device: Device for tensors\n",
        "        \n",
        "    Returns:\n",
        "        x_high: High-dimensional data (n_samples, D)\n",
        "        x_low: Low-dimensional data (n_samples, true_d)\n",
        "        P: Projection matrix (D, true_d)\n",
        "    \"\"\"\n",
        "    if d_type == 'two_moons':\n",
        "        x_low, _ = make_moons(n_samples=n_samples, noise=noise)\n",
        "    else:\n",
        "        x_low, _ = make_swiss_roll(n_samples=n_samples, noise=noise)\n",
        "        x_low = x_low[:,[0,2]]/10\n",
        "\n",
        "    x_low = torch.tensor(x_low, dtype=torch.float32, device=device)\n",
        "\n",
        "    if D == true_d:\n",
        "        P = np.eye(D, dtype=np.float32)\n",
        "        x_high = x_low\n",
        "    else:\n",
        "        # Random orthogonal projection: P in R^(D x d), column-orthogonal\n",
        "        P = torch.randn(D, true_d, dtype=torch.float32)\n",
        "        P, _ = torch.linalg.qr(P)\n",
        "        x_high = x_low @ P.T\n",
        "    \n",
        "    print(f\"Generated data: x_high shape: {x_high.shape}, x_low shape: {x_low.shape}, P shape: {P.shape}\")\n",
        "    \n",
        "    return x_high, x_low, P\n",
        "\n",
        "\n",
        "# Generate datasets for different dimensions\n",
        "n_train = 10000\n",
        "n_test = 5000\n",
        "d = 2  # True dimension\n",
        "dimensions = [2, 8, 16, 512, 1024]\n",
        "\n",
        "datasets = {}\n",
        "for D in dimensions:\n",
        "    print(f\"Generating {D}D dataset...\")\n",
        "    x_train, x_train_low, P_train = generate_manifold_data(n_train, d, D)\n",
        "    x_test, x_test_low, P_test = generate_manifold_data(n_test, d, D)\n",
        "    \n",
        "    datasets[D] = {\n",
        "        'train': (torch.tensor(x_train), torch.tensor(x_train_low), torch.tensor(P_train)),\n",
        "        'test': (torch.tensor(x_test), torch.tensor(x_test_low), torch.tensor(P_test))\n",
        "    }\n",
        "\n",
        "# Visualize 2D data\n",
        "fig, axes = plt.subplots(1, len(dimensions), figsize=(16, 3))\n",
        "for idx, D in enumerate(dimensions):\n",
        "    x_low = datasets[D]['train'][1].numpy()\n",
        "    axes[idx].scatter(x_low[:, 0], x_low[:, 1], s=1, alpha=0.5)\n",
        "    axes[idx].set_title(f'D={D} (projected to 2D)')\n",
        "    axes[idx].set_aspect('equal')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nDataset shapes:\")\n",
        "for D in dimensions:\n",
        "    x_train = datasets[D]['train'][0]\n",
        "    print(f\"  D={D}: {x_train.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Noise Schedule and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import abc\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class Schedule(nn.Module, abc.ABC):\n",
        "    \"\"\"Abstract base class for noise schedules.\"\"\"\n",
        "    \n",
        "    @abc.abstractmethod\n",
        "    def forward(self, t: Tensor) -> tuple[Tensor, Tensor]:\n",
        "        \"\"\"Returns (alpha_t, sigma_t) for time t.\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class LinearSchedule(Schedule):\n",
        "    \"\"\"Linear noise schedule: α_t = t, σ_t = 1-t (t=1 clean, t=0 noise).\"\"\"\n",
        "\n",
        "    def forward(self, t: Tensor) -> tuple[Tensor, Tensor]:\n",
        "        alpha_t = t\n",
        "        sigma_t = 1 - t\n",
        "        return alpha_t, sigma_t\n",
        "\n",
        "\n",
        "# Visualize noise at different time steps\n",
        "schedule = LinearSchedule()\n",
        "t_values = [1.0, 0.85, 0.65, 0.5, 0.0]\n",
        "\n",
        "fig, axes = plt.subplots(len(dimensions), len(t_values), figsize=(16, 12))\n",
        "\n",
        "for row_idx, D in enumerate(dimensions):\n",
        "    x_train = datasets[D]['train'][0]\n",
        "    x_low = datasets[D]['train'][1].numpy()\n",
        "    \n",
        "    for col_idx, t in enumerate(t_values):\n",
        "        t_tensor = torch.tensor([t])\n",
        "        alpha_t, sigma_t = schedule(t_tensor)\n",
        "        epsilon = torch.randn_like(x_train)\n",
        "        x_noisy = alpha_t * x_train + sigma_t * epsilon\n",
        "        \n",
        "        # Project back to 2D for visualization\n",
        "        P = datasets[D]['train'][2].numpy()\n",
        "        x_noisy_2d = x_noisy.numpy() @ P\n",
        "        \n",
        "        axes[row_idx, col_idx].scatter(x_noisy_2d[:, 0], x_noisy_2d[:, 1], s=1, alpha=0.5)\n",
        "        axes[row_idx, col_idx].set_aspect('equal')\n",
        "        \n",
        "        if row_idx == 0:\n",
        "            axes[row_idx, col_idx].set_title(f't={t}\\nα={alpha_t.item():.2f}, σ={sigma_t.item():.2f}', fontsize=10)\n",
        "        if col_idx == 0:\n",
        "            axes[row_idx, col_idx].set_ylabel(f'D={D}', fontsize=12)\n",
        "\n",
        "plt.suptitle('Noisy Samples at Different Noise Levels', fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class predDenoiser(nn.Module):\n",
        "    \"\"\"Wraps backbone with schedule and prediction type (x, ε, or v).\"\"\"\n",
        "\n",
        "    def __init__(self, backbone: nn.Module, schedule: Schedule, denoiser_type: str):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.schedule = schedule\n",
        "        self.denoiser_type = denoiser_type\n",
        "\n",
        "    def forward(self, x_t: Tensor, t: Tensor, **kwargs):\n",
        "        alpha_t, sigma_t = self.schedule(t)\n",
        "        while alpha_t.ndim < x_t.ndim:\n",
        "            alpha_t, sigma_t = alpha_t[..., None], sigma_t[..., None]\n",
        "        return self.backbone(x_t, t, **kwargs)\n",
        "\n",
        "\n",
        "class Loss(nn.Module):\n",
        "    \"\"\"V-loss for all prediction types (converts predictions to velocity).\"\"\"\n",
        "\n",
        "    def __init__(self, l_type: str):\n",
        "        super().__init__()\n",
        "        self.l_type = l_type\n",
        "        self.t_eps = 0.05\n",
        "\n",
        "    def sample_t(self, n, device=None):\n",
        "        \"\"\"Returns the time prior p(t).\"\"\"\n",
        "        return torch.rand(n, 1, device=device)\n",
        "\n",
        "    def forward(self, denoiser, x: Tensor, **kwargs) -> Tensor:\n",
        "        \"\"\"Compute v-loss.\n",
        "        \n",
        "        Args:\n",
        "            denoiser: The denoiser model\n",
        "            x: A clean vector x, with shape (B, ...)\n",
        "            kwargs: Optional keyword arguments\n",
        "\n",
        "        Returns:\n",
        "            The MSE loss between v_pred and v_target\n",
        "        \"\"\"\n",
        "        B, *_ = x.shape\n",
        "        t = self.sample_t(B, device=x.device)\n",
        "        alpha_t, sigma_t = denoiser.schedule(t)\n",
        "        while alpha_t.ndim < x.ndim:\n",
        "            alpha_t, sigma_t = alpha_t[..., None], sigma_t[..., None]\n",
        "\n",
        "        epsilon = torch.randn_like(x)\n",
        "        x_t = alpha_t * x + sigma_t * epsilon\n",
        "        v_t = (x - x_t) / sigma_t.clamp_min(self.t_eps)\n",
        "\n",
        "        # Get raw prediction from backbone\n",
        "        raw_pred = denoiser.backbone(x_t, t)\n",
        "\n",
        "        # Convert prediction to velocity\n",
        "        if denoiser.denoiser_type == 'x':\n",
        "            v_pred = (raw_pred - x_t) / sigma_t.clamp_min(self.t_eps)\n",
        "        elif denoiser.denoiser_type == 'eps':\n",
        "            v_pred = (x_t - raw_pred) / alpha_t.clamp_min(self.t_eps)\n",
        "        elif denoiser.denoiser_type == 'v':\n",
        "            v_pred = raw_pred\n",
        "        \n",
        "        target = v_t\n",
        "        loss = ((v_pred - target)**2).mean()\n",
        "        return loss\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"5-layer MLP backbone with simple time encoding.\"\"\"\n",
        "    \n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.time_encoder = nn.Sequential(\n",
        "            nn.Linear(1, 20), nn.ReLU(),\n",
        "        )\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim + 20, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 256), nn.ReLU(),\n",
        "            nn.Linear(256, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        t_emb = self.time_encoder(t)\n",
        "        out = self.net(torch.cat((x, t_emb), dim=-1))\n",
        "        return out\n",
        "\n",
        "\n",
        "def create_denoiser(pred_type: str, D: int):\n",
        "    \"\"\"Create a denoiser with specified prediction type.\n",
        "    \n",
        "    Args:\n",
        "        pred_type: 'x', 'eps', or 'v'\n",
        "        D: Observed dimension\n",
        "        \n",
        "    Returns:\n",
        "        Denoiser model\n",
        "    \"\"\"\n",
        "    backbone = MLP(dim=D)\n",
        "    schedule = LinearSchedule()\n",
        "    denoiser = predDenoiser(backbone=backbone, schedule=schedule, denoiser_type=pred_type)\n",
        "    return denoiser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_toy_diffusion(\n",
        "    denoiser: nn.Module,\n",
        "    loss_fn: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    n_epochs: int = 200,\n",
        "    lr: float = 1e-3,\n",
        "    device: str = 'cuda'\n",
        "):\n",
        "    \"\"\"Train diffusion model on toy data.\n",
        "    \n",
        "    Args:\n",
        "        denoiser: Denoiser model\n",
        "        loss_fn: Loss function\n",
        "        dataloader: Training data loader\n",
        "        n_epochs: Number of training epochs\n",
        "        lr: Learning rate\n",
        "        device: Device to train on\n",
        "        \n",
        "    Returns:\n",
        "        losses: List of epoch losses\n",
        "    \"\"\"\n",
        "    denoiser = denoiser.to(device)\n",
        "    optimizer = torch.optim.AdamW(denoiser.parameters(), lr=lr)\n",
        "    \n",
        "    losses = []\n",
        "    pbar = tqdm(range(n_epochs), desc=\"Training\")\n",
        "    \n",
        "    for epoch in pbar:\n",
        "        epoch_losses = []\n",
        "        for batch in dataloader:\n",
        "            x = batch[0].to(device)\n",
        "            loss = loss_fn(denoiser, x, cond=None)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_losses.append(loss.item())\n",
        "        \n",
        "        avg_loss = np.mean(epoch_losses)\n",
        "        losses.append(avg_loss)\n",
        "        pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
        "    \n",
        "    return losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 1024\n",
        "n_epochs = 3000\n",
        "lr = 1.0e-3\n",
        "\n",
        "D_MAINS = [2, 8, 16, 512, 1024]\n",
        "pred_types = ['v', 'x', 'eps']\n",
        "\n",
        "results = {}\n",
        "for D_MAIN in D_MAINS:\n",
        "    x_train = datasets[D_MAIN]['train'][0]\n",
        "    train_dataset = TensorDataset(x_train)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    print(f\"Training on D={D_MAIN} with {len(train_dataset)} samples\\n\")\n",
        "\n",
        "    for pred_type in pred_types:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Training {pred_type}-prediction\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        denoiser = create_denoiser(pred_type, D=D_MAIN)\n",
        "        loss_fn = Loss(l_type=pred_type)\n",
        "\n",
        "        losses = train_toy_diffusion(\n",
        "            denoiser=denoiser,\n",
        "            loss_fn=loss_fn,\n",
        "            dataloader=train_loader,\n",
        "            n_epochs=n_epochs,\n",
        "            lr=lr,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        results[f\"{D_MAIN}_{pred_type}\"] = {\n",
        "            'denoiser': denoiser,\n",
        "            'losses': losses\n",
        "        }\n",
        "\n",
        "        print(f\"Final loss: {losses[-1]:.4f}\")\n",
        "\n",
        "    print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for D_MAIN in D_MAINS:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for pred_type in pred_types:\n",
        "        losses = results[f\"{D_MAIN}_{pred_type}\"]['losses']\n",
        "        plt.plot(losses, label=f'{pred_type}-prediction', linewidth=2)\n",
        "    \n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.title(f'Training Curves (D={D_MAIN}, hidden_dim=256, d=2)', fontsize=14)\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.yscale('log')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Final training losses:\")\n",
        "    for pred_type in pred_types:\n",
        "        final_loss = results[f\"{D_MAIN}_{pred_type}\"]['losses'][-1]\n",
        "        print(f\"  {pred_type}-prediction: {final_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Sampling():\n",
        "    \"\"\"Euler ODE sampler using velocity formulation.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        denoiser,\n",
        "        shape,\n",
        "        start: float = 0.0,\n",
        "        stop: float = 1.0,\n",
        "        steps: int = 64,\n",
        "        silent: bool = False,\n",
        "        dtype = None,\n",
        "        device = None,\n",
        "    ):\n",
        "        self.denoiser = denoiser\n",
        "        self.timesteps = torch.linspace(start, stop, steps, dtype=dtype, device=device)\n",
        "        self.silent = silent\n",
        "        self.t_eps = 0.05\n",
        "        self.init_x = torch.randn(shape, dtype=dtype, device=device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, x0: Tensor, **kwargs) -> Tensor:\n",
        "        x_t = x0\n",
        "        for i in tqdm(range(len(self.timesteps) - 1), desc=\"Sampling\", disable=self.silent):\n",
        "            t = self.timesteps[i]\n",
        "            s = self.timesteps[i + 1]\n",
        "            x_t = self.step(x_t, t=t, s=s, **kwargs)\n",
        "        return x_t\n",
        "\n",
        "    def step(self, x_t: Tensor, t: Tensor, s: Tensor, **kwargs) -> Tensor:\n",
        "        \"\"\"Euler ODE step using velocity formulation.\n",
        "\n",
        "        Arguments:\n",
        "            x_t: The current tensor x_t\n",
        "            t: The current time t\n",
        "            s: The target time s\n",
        "            kwargs: Optional keyword arguments\n",
        "\n",
        "        Returns:\n",
        "            The new tensor x_s\n",
        "        \"\"\"\n",
        "        t = t.expand_as(x_t[..., 0])\n",
        "        s = s.expand_as(x_t[..., 0])\n",
        "\n",
        "        alpha_t, sigma_t = self.denoiser.schedule(t)\n",
        "\n",
        "        while alpha_t.ndim < x_t.ndim:\n",
        "            alpha_t, sigma_t = alpha_t[..., None], sigma_t[..., None]\n",
        "        while t.ndim < x_t.ndim:\n",
        "            t = t[..., None]\n",
        "            s = s[..., None]\n",
        "\n",
        "        raw_pred = self.denoiser.backbone(x_t, t, **kwargs)\n",
        "        \n",
        "        if self.denoiser.denoiser_type == 'x':\n",
        "            v_pred = (raw_pred - x_t) / (sigma_t).clamp_min(self.t_eps)\n",
        "        elif self.denoiser.denoiser_type == 'eps':\n",
        "            v_pred = (x_t - raw_pred) / alpha_t.clamp_min(self.t_eps)\n",
        "        elif self.denoiser.denoiser_type == 'v':\n",
        "            v_pred = raw_pred\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown denoiser type: {self.denoiser.denoiser_type}\")\n",
        "\n",
        "        return x_t + v_pred * (s - t)\n",
        "\n",
        "\n",
        "def sample_toy_diffusion(\n",
        "    denoiser: nn.Module,\n",
        "    n_samples: int,\n",
        "    dim: int,\n",
        "    n_steps: int = 256,\n",
        "    device: str = 'cuda'\n",
        "):\n",
        "    \"\"\"Sample from trained toy diffusion model.\n",
        "    \n",
        "    Args:\n",
        "        denoiser: Trained denoiser model\n",
        "        n_samples: Number of samples to generate\n",
        "        dim: Observed dimension\n",
        "        n_steps: Number of sampling steps\n",
        "        device: Device to use\n",
        "    \"\"\"\n",
        "    denoiser = denoiser.to(device)\n",
        "    \n",
        "    sampler = Sampling(\n",
        "        denoiser=denoiser, \n",
        "        shape=(n_samples, dim), \n",
        "        steps=n_steps, \n",
        "        device=device\n",
        "    )\n",
        "    \n",
        "    x0 = sampler.init_x\n",
        "    x1 = sampler(x0)\n",
        "    x1 = x1.reshape(n_samples, dim)\n",
        "\n",
        "    return x1.cpu()\n",
        "\n",
        "\n",
        "# Generate samples from all models\n",
        "samples = {}\n",
        "for D_MAIN in D_MAINS:\n",
        "    n_samples = 2000\n",
        "    n_steps = 51\n",
        "\n",
        "    for pred_type in pred_types:\n",
        "        print(f\"Sampling from {pred_type}-prediction (D={D_MAIN})...\")\n",
        "        denoiser = results[f\"{D_MAIN}_{pred_type}\"]['denoiser']\n",
        "        samples[f\"{D_MAIN}_{pred_type}\"] = sample_toy_diffusion(\n",
        "            denoiser=denoiser,\n",
        "            n_samples=n_samples,\n",
        "            dim=D_MAIN,\n",
        "            n_steps=n_steps,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "    print(\"Sampling complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project samples to 2D\n",
        "samples_2d = {}\n",
        "for D_MAIN in D_MAINS:\n",
        "    for pred_type in pred_types:\n",
        "        P = datasets[D_MAIN]['train'][2].numpy()\n",
        "        samples_high = samples[f\"{D_MAIN}_{pred_type}\"].numpy()\n",
        "        samples_2d[f\"{D_MAIN}_{pred_type}\"] = samples_high @ P\n",
        "\n",
        "# Plot samples\n",
        "colors = {'x': 'blue', 'eps': 'red', 'v': 'green'}\n",
        "for D_MAIN in D_MAINS:\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(16, 4), width_ratios=[1, 1, 1, 1])\n",
        "    \n",
        "    x_test_low = datasets[D_MAIN]['test'][1].numpy()\n",
        "\n",
        "    # True data\n",
        "    axes[0].scatter(x_test_low[:, 0], x_test_low[:, 1], s=1, alpha=0.5, c='black')\n",
        "    axes[0].set_title('True Data', fontsize=14)\n",
        "    axes[0].set_aspect('equal')\n",
        "    axes[0].set_xlim(-1.5, 1.5)\n",
        "    axes[0].set_ylim(-1.5, 1.5)\n",
        "\n",
        "    for idx, pred_type in enumerate(pred_types):\n",
        "        s = samples_2d[f\"{D_MAIN}_{pred_type}\"]\n",
        "        axes[idx + 1].scatter(s[:, 0], s[:, 1], s=1, alpha=0.5, c=colors[pred_type])\n",
        "        axes[idx + 1].set_title(f'{pred_type}-prediction', fontsize=14)\n",
        "        axes[idx + 1].set_aspect('equal')\n",
        "        axes[idx + 1].set_xlim(-1.5, 1.5)\n",
        "        axes[idx + 1].set_ylim(-1.5, 1.5)\n",
        "\n",
        "    plt.suptitle(f'Generated Samples (D={D_MAIN}, projected to 2D)', fontsize=16, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
